# Auto Sales Analytics Pipeline

## ğŸ¯ Project Overview
Production-ready data pipeline built on Databricks with CI/CD automation using GitHub Actions.

### Pipeline Architecture
```
CSV Data â†’ Bronze Layer â†’ Silver Layer â†’ Gold Layer â†’ Analytics Tables
```

**Medallion Architecture:**
- **Bronze Layer**: Raw data ingestion (2,747 sales records)
- **Silver Layer**: Data cleaning and transformation
- **Gold Layer**: Business analytics (5 tables)

## ğŸ“Š Analytics Tables Created
1. `gold_sales_by_product_line` - Product performance metrics
2. `gold_sales_by_country` - Geographic sales analysis
3. `gold_monthly_sales_trends` - Time-series analysis
4. `gold_deal_size_analysis` - Deal segmentation
5. `gold_executive_dashboard` - Executive KPIs

## ğŸš€ Features
- âœ… Automated CI/CD pipeline with GitHub Actions
- âœ… Multi-environment deployment (DEV/STAGING/PROD)
- âœ… Data quality monitoring
- âœ… Complete data lineage tracking
- âœ… Delta Lake for ACID transactions
- âœ… Unity Catalog integration

## ğŸ—ï¸ Repository Structure
```
auto-sales-databricks-pipeline/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ databricks-cicd.yml    # GitHub Actions workflow
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_bronze_ingestion.py     # Bronze layer notebook
â”‚   â”œâ”€â”€ 02_silver_cleaning.py      # Silver layer notebook
â”‚   â””â”€â”€ 03_gold_analytics.py       # Gold layer notebook
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dev.json                   # Development environment config
â”‚   â”œâ”€â”€ staging.json               # Staging environment config
â”‚   â””â”€â”€ prod.json                  # Production environment config
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_data_quality.py       # Automated tests
â”œâ”€â”€ .gitignore                      # Git ignore rules
â””â”€â”€ README.md                       # This file
```

## ğŸ”§ Setup Instructions

### 1. GitHub Secrets Configuration
Add these secrets in GitHub Settings â†’ Secrets â†’ Actions:
- `DATABRICKS_HOST`: Your Databricks workspace URL
- `DATABRICKS_TOKEN`: Personal access token from Databricks

### 2. Databricks Setup
1. Create three catalogs/schemas: `dev`, `staging`, `prod`
2. Upload source data to Unity Catalog volume
3. Configure cluster policies

### 3. Running the Pipeline

**Manually in Databricks:**
```python
# Run notebooks in sequence:
1. 01_bronze_ingestion.py
2. 02_silver_cleaning.py
3. 03_gold_analytics.py
```

**Via Databricks Job:**
- Job Name: "Auto Sales Analytics Pipeline"
- Runs all 3 notebooks in sequence
- Duration: ~1-2 minutes

**Via CI/CD (Automated):**
- Push to `develop` branch â†’ Deploys to DEV
- Push to `main` branch â†’ Deploys to PROD

## ğŸ“ˆ Key Metrics
- **Total Records Processed**: 2,747
- **Pipeline Run Time**: 1m 14s
- **Data Quality Score**: 99%+
- **Number of Analytics Tables**: 5

## ğŸ”„ CI/CD Pipeline Flow
```
Code Commit â†’ GitHub Actions Triggered â†’ Run Tests â†’ Deploy to Environment â†’ Verify Deployment
```

## ğŸ‘¥ Team
- **Developer**: [Your Name]
- **Project**: Databricks Data Engineering Demo

## ğŸ“ License
Internal Use Only
